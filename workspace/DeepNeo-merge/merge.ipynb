{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, timeit, gzip, glob, numpy, math, cPickle, subprocess, math\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from theano.tensor.signal import pool\n",
    "from theano.tensor.nnet import conv2d\n",
    "from mlp import relu, HiddenLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matchDat(afflst, hladic, aadic):\n",
    "\tseqlst = []\n",
    "\ttablst = []\n",
    "\theader = []\n",
    "\tfor affin in afflst:\n",
    "\t\taffstr = affin.strip().split('\\t')\n",
    "\t\tif affstr[0] in hladic:\n",
    "\t\t\thlaseq = hladic[affstr[0]]\n",
    "\t\t\taaseq = affstr[1]\n",
    "\t\t\ttmp = []\n",
    "\t\t\ttmp0 = []\n",
    "\t\t\tfor hlain in hlaseq:\n",
    "\t\t\t\tfor aain in aaseq:\n",
    "\t\t\t\t\tif hlain == 'X' or aain=='X':\n",
    "\t\t\t\t\t\ttmp0.append([float(0)])\n",
    "\t\t\t\t\telif hlain == '*':\n",
    "\t\t\t\t\t\ttmp0.append([float(0)])\n",
    "\t\t\t\t\telif hlain == '.':\n",
    "\t\t\t\t\t\ttmp0.append([float(0)])\n",
    "\t\t\t\t\telif aain == 'X':\n",
    "\t\t\t\t\t\ttmp0.append([float(0)])\n",
    "\t\t\t\t\telif aain == 'U':\n",
    "\t\t\t\t\t\ttmp0.append([aadic[hlain, 'C']])\n",
    "\t\t\t\t\telif aain == 'J':\n",
    "\t\t\t\t\t\taa1 = aadic[hlain, 'L']\n",
    "\t\t\t\t\t\taa2 = aadic[hlain, 'I']\n",
    "\t\t\t\t\t\taamax = max(aa1, aa2)\n",
    "\t\t\t\t\t\ttmp0.append([float(aamax)])\n",
    "\t\t\t\t\telif aain == 'Z':\n",
    "\t\t\t\t\t\taa1 = aadic[hlain, 'Q']\n",
    "\t\t\t\t\t\taa2 = aadic[hlain, 'E']\n",
    "\t\t\t\t\t\taamax = max(aa1, aa2)\n",
    "\t\t\t\t\t\ttmp0.append([float(aamax)])\n",
    "\t\t\t\t\telif aain  == 'B':\n",
    "\t\t\t\t\t\taa1 = aadic[hlain, 'D']\n",
    "\t\t\t\t\t\taa2 = aadic[hlain, 'N']\n",
    "\t\t\t\t\t\taamax = max(aa1, aa2)\n",
    "\t\t\t\t\t\ttmp0.append([float(aamax)])\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\ttmp0.append([aadic[hlain, aain]])\n",
    "\t\t\t\ttmp.append(tmp0)\n",
    "\t\t\t\ttmp0 = []\n",
    "\t\t\tseqlst.append(zip(*tmp))\n",
    "\t\t\ttablst.append(int(affstr[2]))\n",
    "\t\t\theader.append((affstr[0], affstr[1]))\n",
    "\tseqarray0 = np.array(seqlst, dtype = theano.config.floatX)\n",
    "\tdel seqlst\n",
    "\ta_seq2 = seqarray0.reshape(seqarray0.shape[0], seqarray0.shape[1] * seqarray0.shape[2])\n",
    "\ta_lab2 = np.array(tablst, dtype = theano.config.floatX)\n",
    "\tdel tablst\n",
    "\treturn ((a_seq2, a_lab2)), header\n",
    "\tdel a_seq2, a_lab2, header\n",
    "\n",
    "def HeaderOutput(lstin, outname):\n",
    "\toutw = open(outname, 'w')\n",
    "\tfor lin in lstin:\n",
    "\t\toutw.write('\\t'.join(lin)+'\\n')\n",
    "\toutw.close()\n",
    "\n",
    "def modifyMatrix(affydatin_test, seqdatin,outfile):\n",
    "\thladicin = {x.strip().split('\\t')[0]: x.strip().split('\\t')[1] for x in open(seqdatin).readlines()}\n",
    "\taalst = open('data/Calpha.txt').readlines()\n",
    "\taadicin = {}\n",
    "\taaseq0 = aalst[0].strip().split('\\t')\n",
    "\tfor aain in aalst[1:]:\n",
    "\t\taastr = aain.strip().split('\\t' )\n",
    "\t\tfor i in range(1, len(aastr)):\n",
    "\t\t\taadicin[aaseq0[i-1], aastr[0]] = float(aastr[i])\n",
    "\tafflst = open(affydatin_test).readlines()\n",
    "\td, test_header = matchDat(afflst, hladicin, aadicin)\n",
    "\toutname0 = affydatin_test\n",
    "\toutname2 = affydatin_test+'.header'\n",
    "\t#np.savez_compressed(outname0, test_seq = test_seq, test_lab = test_lab)\n",
    "        cPickle.dump(d, gzip.open(outfile, 'wb'), protocol = 2)\n",
    "\tHeaderOutput(test_header, outname2)\n",
    "\n",
    "Datname = 'data/class1_input.dat'\n",
    "mhcclass = 'class1'\n",
    "outputfile = 'temp/class1_input.dat.pkl.gz'\n",
    "\n",
    "print 'Input file:', Datname\n",
    "modifyMatrix(Datname, 'data/All_prot_alignseq_C_369.dat',outputfile)\n",
    "# modifyMatrix(Datname, 'data/MHC2_prot_alignseq.dat',outputfile)\n",
    "print 'The running is completed!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import pylab\n",
    "\n",
    "def random_mixture_model(pos_mu=.6,pos_sigma=.1,neg_mu=.4,neg_sigma=.1,size=200):\n",
    "\tpos = [(1,random.gauss(pos_mu,pos_sigma),) for x in xrange(size/2)]\n",
    "\tneg = [(0,random.gauss(neg_mu,neg_sigma),) for x in xrange(size/2)]\n",
    "\treturn pos+neg\n",
    "\n",
    "def plot_multiple_rocs_separate(rocList,title='', labels = None, equal_aspect = True):\n",
    "\t\"\"\" Plot multiples ROC curves as separate at the same painting area. \"\"\"\n",
    "\tpylab.clf()\n",
    "\tpylab.title(title)\n",
    "\tfor ix, r in enumerate(rocList):\n",
    "\t\tax = pylab.subplot(4,4,ix+1)\n",
    "\t\tpylab.ylim((0,1))\n",
    "\t\tpylab.xlim((0,1))\n",
    "\t\tax.set_yticklabels([])\n",
    "\t\tax.set_xticklabels([])\n",
    "\t\tif equal_aspect:\n",
    "\t\t\tcax = pylab.gca()\n",
    "\t\t\tcax.set_aspect('equal')\n",
    "\t\t\n",
    "\t\tif not labels:\n",
    "\t\t\tlabels = ['' for x in rocList]\n",
    "\t\t\n",
    "\t\tpylab.text(0.2,0.1,labels[ix],fontsize=8)\n",
    "\t\tpylab.plot([x[0] for x in r.derived_points],[y[1] for y in r.derived_points], 'r-',linewidth=2)\n",
    "\t\n",
    "\tpylab.show()\n",
    "\t\n",
    "def _remove_duplicate_styles(rocList):\n",
    " \t\"\"\" Checks for duplicate linestyles and replaces duplicates with a random one.\"\"\"\n",
    "\tpref_styles = ['cx-','mx-','yx-','gx-','bx-','rx-']\n",
    "\tpoints = 'ov^>+xd'\n",
    "\tcolors = 'bgrcmy'\n",
    "\tlines = ['-','-.',':']\n",
    "\t\n",
    "\trand_ls = []\n",
    "\t\n",
    "\tfor r in rocList:\n",
    "\t\tif r.linestyle not in rand_ls:\n",
    "\t\t\trand_ls.append(r.linestyle)\n",
    "\t\telse:\n",
    "\t\t\twhile True:\n",
    "\t\t\t\tif len(pref_styles) > 0:\n",
    "\t\t\t\t\tpstyle = pref_styles.pop()\n",
    "\t\t\t\t\tif pstyle not in rand_ls:\n",
    "\t\t\t\t\t\tr.linestyle = pstyle\n",
    "\t\t\t\t\t\trand_ls.append(pstyle)\n",
    "\t\t\t\t\t\tbreak\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tls = ''.join(random.sample(colors,1) + random.sample(points,1)+ random.sample(lines,1))\n",
    "\t\t\t\t\tif ls not in rand_ls:\n",
    "\t\t\t\t\t\tr.linestyle = ls\n",
    "\t\t\t\t\t\trand_ls.append(ls)\n",
    "\t\t\t\t\t\tbreak\n",
    "\t\t\t\t\t\t\n",
    "def plot_multiple_roc(rocList,title='',labels=None, include_baseline=False, equal_aspect=True):\n",
    "\tpylab.clf()\n",
    "\tpylab.ylim((0,1))\n",
    "\tpylab.xlim((0,1))\n",
    "\tpylab.xticks(pylab.arange(0,1.1,.1))\n",
    "\tpylab.yticks(pylab.arange(0,1.1,.1))\n",
    "\tpylab.grid(True)\n",
    "\tif equal_aspect:\n",
    "\t\tcax = pylab.gca()\n",
    "\t\tcax.set_aspect('equal')\n",
    "\tpylab.xlabel(\"1 - Specificity\")\n",
    "\tpylab.ylabel(\"Sensitivity\")\n",
    "\tpylab.title(title)\n",
    "\tif not labels:\n",
    "\t\tlabels = [ '' for x in rocList]\n",
    "\t_remove_duplicate_styles(rocList)\n",
    "\tfor ix, r in enumerate(rocList):\n",
    "\t\tpylab.plot([x[0] for x in r.derived_points], [y[1] for y in r.derived_points], r.linestyle, linewidth=1, label=labels[ix])\n",
    "\tif include_baseline:\n",
    "\t\tpylab.plot([0.0,1.0], [0.0, 1.0], 'k-', label= 'random')\n",
    "\tif labels:\n",
    "\t\tpylab.legend(loc='lower right')\n",
    "\t\t\n",
    "\tpylab.show()\n",
    "\n",
    "def load_decision_function(path):\n",
    "\tfileHandler = open(path,'r')\n",
    "\treader = fileHandler.readlines()\n",
    "\treader = [line.strip().split() for line in reader]\n",
    "\tmodel_data = []\n",
    "\tfor line in reader:\n",
    "\t\tif len(line) == 0: continue\n",
    "\t\tfClass,fValue = line\n",
    "\t\tmodel_data.append((int(fClass), float(fValue)))\n",
    "\tfileHandler.close()\n",
    "\n",
    "\treturn model_data\n",
    "\t\n",
    "class ROCData(object):\n",
    "\tdef __init__(self,data,linestyle='rx-'):\n",
    "\t\tself.data = sorted(data,lambda x,y: cmp(y[1],x[1]))\n",
    "\t\tself.linestyle = linestyle\n",
    "\t\tself.auc() #Seed initial points with default full ROC\n",
    "\t\n",
    "\tdef auc(self,fpnum=0):\n",
    "\t\tfps_count = 0\n",
    "\t\trelevant_pauc = []\n",
    "\t\tcurrent_index = 0\n",
    "\t\tmax_n = len([x for x in self.data if x[0] == 0])\n",
    "\t\tif fpnum == 0:\n",
    "\t\t\trelevant_pauc = [x for x in self.data]\n",
    "\t\telif fpnum > max_n:\n",
    "\t\t\tfpnum = max_n\n",
    "\t\t#Find the upper limit of the data that does not exceed n FPs\n",
    "\t\telse:\n",
    "\t\t\twhile fps_count < fpnum:\n",
    "\t\t\t\trelevant_pauc.append(self.data[current_index])\n",
    "\t\t\t\tif self.data[current_index][0] == 0:\n",
    "\t\t\t\t\tfps_count += 1\n",
    "\t\t\t\tcurrent_index +=1\n",
    "\t\ttotal_n = len([x for x in relevant_pauc if x[0] == 0])\n",
    "\t\ttotal_p = len(relevant_pauc) - total_n\n",
    "\t\t\n",
    "\t\t#Convert to points in a ROC\n",
    "\t\tprevious_df = -1000000.0\n",
    "\t\tcurrent_index = 0\n",
    "\t\tpoints = []\n",
    "\t\ttp_count, fp_count = 0.0 , 0.0\n",
    "\t\ttpr, fpr = 0, 0\n",
    "\t\twhile current_index < len(relevant_pauc):\n",
    "\t\t\tdf = relevant_pauc[current_index][1]\n",
    "\t\t\tif previous_df != df:\n",
    "\t\t\t\tpoints.append((fpr,tpr,fp_count))\n",
    "\t\t\tif relevant_pauc[current_index][0] == 0:\n",
    "\t\t\t\tfp_count +=1\n",
    "\t\t\telif relevant_pauc[current_index][0] == 1:\n",
    "\t\t\t\ttp_count +=1\n",
    "\t\t\tfpr = fp_count/total_n\n",
    "\t\t\ttpr = tp_count/total_p\n",
    "\t\t\tprevious_df = df\n",
    "\t\t\tcurrent_index +=1\n",
    "\t\tpoints.append((fpr,tpr,fp_count)) #Add last point\n",
    "\t\tpoints.sort(key=lambda i: (i[0],i[1]))\n",
    "\t\tself.derived_points = points\n",
    "\t\t\n",
    "\t\treturn self._trapezoidal_rule(points)\n",
    "\n",
    "\n",
    "\tdef _trapezoidal_rule(self,curve_pts):\n",
    "\t\tcum_area = 0.0\n",
    "\t\tfor ix,x in enumerate(curve_pts[0:-1]):\n",
    "\t\t\tcur_pt = x\n",
    "\t\t\tnext_pt = curve_pts[ix+1]\n",
    "\t\t\tcum_area += ((cur_pt[1]+next_pt[1])/2.0) * (next_pt[0]-cur_pt[0])\n",
    "\t\treturn cum_area\n",
    "\t\t\n",
    "\tdef calculateStandardError(self,fpnum=0):\n",
    "\t\tarea = self.auc(fpnum)\n",
    "\t\t\n",
    "\t\t#real positive cases\n",
    "\t\tNa =  len([ x for x in self.data if x[0] == 1])\n",
    "\t\t\n",
    "\t\t#real negative cases\n",
    "\t\tNn =  len([ x for x in self.data if x[0] == 0])\n",
    "\t\t\n",
    "\t\t\n",
    "\t\tQ1 = area / (2.0 - area)\n",
    "\t\tQ2 = 2 * area * area / (1.0 + area)\n",
    "\t\t\n",
    "\t\treturn math.sqrt( ( area * (1.0 - area)  +   (Na - 1.0) * (Q1 - area*area) +\n",
    "\t\t\t\t\t\t(Nn - 1.0) * (Q2 - area * area)) / (Na * Nn))\n",
    "\t\t\t\t\t\t\t\n",
    "\t\n",
    "\tdef plot(self,title='',include_baseline=False,equal_aspect=True):\t\t\n",
    "\t\tpylab.clf()\n",
    "\t\tpylab.plot([x[0] for x in self.derived_points], [y[1] for y in self.derived_points], self.linestyle)\n",
    "\t\tif include_baseline:\n",
    "\t\t\tpylab.plot([0.0,1.0], [0.0,1.0],'k-.')\n",
    "\t\tpylab.ylim((0,1))\n",
    "\t\tpylab.xlim((0,1))\n",
    "\t\tpylab.xticks(pylab.arange(0,1.1,.1))\n",
    "\t\tpylab.yticks(pylab.arange(0,1.1,.1))\n",
    "\t\tpylab.grid(True)\n",
    "\t\tif equal_aspect:\n",
    "\t\t\tcax = pylab.gca()\n",
    "\t\t\tcax.set_aspect('equal')\n",
    "\t\tpylab.xlabel('1 - Specificity')\n",
    "\t\tpylab.ylabel('Sensitivity')\n",
    "\t\tpylab.title(title)\n",
    "#\t\tpylab.show()\n",
    "                pylab.savefig(title+'.png')\n",
    "\t\t\n",
    "\t\n",
    "\tdef confusion_matrix(self,threshold,do_print=False):\n",
    "\t\tpos_points = [x for x in self.data if x[1] >= threshold]\n",
    "\t\tneg_points = [x for x in self.data if x[1] < threshold]\n",
    "\t\ttp,fp,fn,tn = self._calculate_counts(pos_points,neg_points)\n",
    "\t\tif do_print:\n",
    "\t\t\tprint \"\\t Actual class\"\n",
    "\t\t\tprint \"\\t+(1)\\t-(0)\"\n",
    "\t\t\tprint \"+(1)\\t%i\\t%i\\tPredicted\" % (tp,fp)\n",
    "\t\t\tprint \"-(0)\\t%i\\t%i\\tclass\" % (fn,tn)\n",
    "\t\treturn {'TP': tp, 'FP': fp, 'FN': fn, 'TN': tn}\n",
    "\t\t\n",
    "\n",
    "\t\n",
    "\tdef evaluateMetrics(self,matrix,metric=None,do_print=False):\n",
    "\t\taccuracy = (matrix['TP'] + matrix['TN'])/ float(sum(matrix.values()))\n",
    "\t\tsensitivity = (matrix['TP'])/ float(matrix['TP'] + matrix['FN'])\n",
    "\t\tspecificity = (matrix['TN'])/float(matrix['TN'] + matrix['FP'])\n",
    "\t\tefficiency = (sensitivity + specificity) / 2.0\n",
    "\t\tpositivePredictiveValue =  matrix['TP'] / float(matrix['TP'] + matrix['FP'])\n",
    "\t\tNegativePredictiveValue = matrix['TN'] / float(matrix['TN'] + matrix['FN'])\n",
    "\t\tPhiCoefficient = (matrix['TP'] * matrix['TN'] - matrix['FP'] * matrix['FN'])/(\n",
    "\t\t\t\t\t\t\tmath.sqrt( (matrix['TP'] + matrix['FP']) *\n",
    "\t\t\t\t\t\t\t           (matrix['TP'] + matrix['FN']) *\n",
    "\t\t\t\t\t\t\t\t\t   (matrix['TN'] + matrix['FP']) *\n",
    "\t\t\t\t\t\t\t\t\t   (matrix['TN'] + matrix['FN']))) or 1.0\n",
    "\t\t\t\t\t\t\t\t\t\n",
    "\t\tif do_print:\n",
    "\t\t\tprint 'Sensitivity: ' , sensitivity\n",
    "\t\t\tprint 'Specificity: ' , specificity\n",
    "\t\t\tprint 'Efficiency: ' , efficiency\n",
    "\t\t\tprint 'Accuracy: ' , accuracy\n",
    "\t\t\tprint 'PositivePredictiveValue: ' , positivePredictiveValue\n",
    "\t\t\tprint 'NegativePredictiveValue' , NegativePredictiveValue\n",
    "\t\t\tprint 'PhiCoefficient' , PhiCoefficient\n",
    "\t\t\t\n",
    "\t\treturn {'SENS': sensitivity, 'SPEC': specificity, 'ACC': accuracy, 'EFF': efficiency,\n",
    "\t\t\t\t'PPV':positivePredictiveValue, 'NPV':NegativePredictiveValue , 'PHI':  PhiCoefficient}\n",
    "\t\n",
    "\tdef _calculate_counts(self,pos_data,neg_data):\n",
    "\t\t\"\"\" Calculates the number of false positives, true positives, false negatives and true negatives \"\"\"\n",
    "\t\ttp_count = len([x for x in pos_data if x[0] == 1])\n",
    "\t\tfp_count = len([x for x in pos_data if x[0] == 0])\n",
    "\t\tfn_count = len([x for x in neg_data if x[0] == 1])\n",
    "\t\ttn_count = len([x for x in neg_data if x[0] == 0])\n",
    "\t\treturn tp_count,fp_count,fn_count, tn_count\n",
    "\t\t\n",
    "if __name__ == '__main__':\n",
    "\tprint \"PyRoC - ROC Curve Generator\"\n",
    "\tfrom optparse import OptionParser\n",
    "\t\n",
    "\tparser = OptionParser()\n",
    "\tparser.add_option('-f', '--file', dest='origFile', help=\"Path to a file with the class and decision function. The first column of each row is the class, and the second the decision score.\")\n",
    "\tparser.add_option(\"-n\", \"--max fp\", dest = \"fp_n\", default=0, help= \"Maximum false positives to calculate up to (for partial AUC).\")\n",
    "\tparser.add_option(\"-p\",\"--plot\", action=\"store_true\",dest='plotFlag', default=False, help=\"Plot the ROC curve (matplotlib required)\")\n",
    "\tparser.add_option(\"-t\",'--title', dest= 'ptitle' , default='' , help = 'Title of plot.')\n",
    "\t\n",
    "\t(options,args) = parser.parse_args()\n",
    "\n",
    "\tif (not options.origFile):\n",
    "\t\tparser.print_help()\n",
    "\t\texit()\n",
    "\n",
    "\tdf_data = load_decision_function(options.origFile)\n",
    "\troc = ROCData(df_data)\n",
    "\troc_n = int(options.fp_n)\n",
    "\tprint \"ROC AUC: %s\" % (str(roc.auc(roc_n)),)\n",
    "\tprint 'Standard Error:  %s' % (str(roc.calculateStandardError(roc_n)),) \n",
    "\t\n",
    "\tprint ''\n",
    "\tfor pt in roc.derived_points:\n",
    "\t\tprint pt[0],pt[1]\n",
    "\t\t\n",
    "\tif options.plotFlag:\n",
    "\t\troc.plot(options.ptitle,True,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shared_dataset(data_xy, borrow=True):\n",
    "    data_x, data_y = data_xy\n",
    "    shared_x = theano.shared(np.asarray(data_x, dtype=theano.config.floatX), borrow=borrow)\n",
    "    shared_y = theano.shared(np.asarray(data_y, dtype=theano.config.floatX), borrow=borrow)\n",
    "    return shared_x, T.cast(shared_y, 'int32')\n",
    "\n",
    "def Load_data(dataset):\n",
    "    print '... loading data'\n",
    "    train_set, valid_set, test_set = cPickle.load( gzip.open(dataset, 'rb') )\n",
    "    train_set_x, train_set_y = shared_dataset(train_set)\n",
    "    test_set_x, test_set_y   = shared_dataset(test_set)\n",
    "    valid_set_x, valid_set_y = shared_dataset(valid_set)\n",
    "    rval = [(train_set_x, train_set_y), (valid_set_x, valid_set_y), (test_set_x, test_set_y)]\n",
    "    return rval\n",
    "\n",
    "def Load_data_ind(dataset):\n",
    "    print '... loading data'\n",
    "    test_set = cPickle.load( gzip.open(dataset, 'rb') )\n",
    "    test_set_x, test_set_y   = shared_dataset(test_set)\n",
    "    return [(test_set_x, test_set_y)]\n",
    "\n",
    "def Load_npdata(dataset):\n",
    "    print '... loading data'\n",
    "    datasets = np.load(dataset)\n",
    "    test_setx = datasets['test_seq']\n",
    "    test_sety = datasets['test_lab']\n",
    "    test_set = (test_setx, test_sety)\n",
    "    test_set_x, test_set_y   = shared_dataset(test_set)\n",
    "    return [(test_set_x, test_set_y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNetConvPoolLayer(object):\n",
    "    def __init__(self, rng, input, filter_shape, image_shape, poolsize):\n",
    "        assert image_shape[1] == filter_shape[1]\n",
    "        self.input = input\n",
    "        fan_in  = numpy.prod(filter_shape[1:])\n",
    "        fan_out = (filter_shape[0] * numpy.prod(filter_shape[2:]))\n",
    "        W_bound = numpy.sqrt(10. / (fan_in + fan_out))  \n",
    "        self.W  = theano.shared(\n",
    "            numpy.asarray(\n",
    "                rng.uniform(low=-W_bound, high=W_bound, size=filter_shape),\n",
    "                dtype=theano.config.floatX\n",
    "            ),  borrow=True\n",
    "        )\n",
    "        b_values = numpy.zeros((filter_shape[0],), dtype=theano.config.floatX)\n",
    "        self.b   = theano.shared(value=b_values, borrow=True)\n",
    "        conv_out = conv2d(\n",
    "            input=input,\n",
    "            filters=self.W,\n",
    "            filter_shape=filter_shape,\n",
    "            input_shape=image_shape\n",
    "        )\n",
    "        self.output = T.nnet.relu(conv_out + self.b.dimshuffle('x', 0, 'x', 'x'))\n",
    "        self.params = [self.W, self.b]\n",
    "        self.input = input\n",
    "\n",
    "class LogisticRegression(object):\n",
    "    def __init__(self, input, n_in, n_out, W=None, b=None):\n",
    "        if W is None:\n",
    "            W = theano.shared(\n",
    "                value=numpy.zeros(\n",
    "                    (n_in, n_out),\n",
    "                    dtype=theano.config.floatX\n",
    "                ),name='W',borrow=True\n",
    "            )\n",
    "        if b is None:\n",
    "            b = theano.shared(\n",
    "                value=numpy.zeros((n_out,),\n",
    "                    dtype=theano.config.floatX\n",
    "                ),name='b',borrow=True\n",
    "            )\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.output = T.nnet.sigmoid(T.dot(input, self.W)+ self.b).flatten()\n",
    "        self.p_y_given_x = T.nnet.sigmoid(T.dot(input, self.W)+ self.b).flatten()\n",
    "        self.y_pred = self.output > .5\n",
    "        self.params = [self.W, self.b]\n",
    "        self.x = input\n",
    "    def negative_log_likelihood(self, y):\n",
    "        return - T.mean(y * T.log(self.output) + (1 - y) * T.log(1. - self.output))\n",
    "\n",
    "class CNN(object):\n",
    "    def __init__(self, rng=None, nkerns=None, batch_size=None, in_dim=None, filtsize=None, poolsize=None, hidden=None):\n",
    "        self.layers = []\n",
    "        self.params = []\n",
    "        self.x = T.matrix('x')\n",
    "        self.y = T.ivector('y')\n",
    "        layer0_input = self.x.reshape((batch_size, 1, in_dim[0], in_dim[1]))\n",
    "        layer0 = LeNetConvPoolLayer(\n",
    "            rng,\n",
    "            input=layer0_input,\n",
    "            image_shape=(batch_size, 1, in_dim[0], in_dim[1]),\n",
    "            filter_shape=(nkerns[0], 1, filtsize[0][0], filtsize[0][1]),\n",
    "            poolsize=poolsize[0]\n",
    "        )\n",
    "        self.layers.append(layer0)\n",
    "        dim11 = (in_dim[0]-filtsize[0][0] +1)\n",
    "        dim12 = (in_dim[1]-filtsize[0][1] +1)\n",
    "        layer1 = LeNetConvPoolLayer(\n",
    "            rng,\n",
    "            input=layer0.output,\n",
    "            image_shape=(batch_size, nkerns[0], dim11, dim12),\n",
    "            filter_shape=(nkerns[1], nkerns[0], filtsize[1][0], filtsize[1][1]),\n",
    "            poolsize=poolsize[1]\n",
    "        )\n",
    "        self.layers.append(layer1)\n",
    "        dim21 = (dim11 - filtsize[1][0] +1)\n",
    "        dim22 = (dim12 - filtsize[1][1] +1)\n",
    "        layer2_input = layer1.output.flatten(2)\n",
    "        layer2 = LogisticRegression(input=layer2_input, n_in=hidden, n_out=1)\n",
    "        self.layers.append(layer2)\n",
    "        self.params = [ param for layer in self.layers for param in layer.params ]\n",
    "        self.gparams_mom = []\n",
    "        for param in self.params:\n",
    "            gparam_mom = theano.shared(numpy.zeros(param.get_value(borrow=True).shape,\n",
    "            dtype=theano.config.floatX))\n",
    "            self.gparams_mom.append(gparam_mom)\n",
    "        self.finetune_cost = layer2.negative_log_likelihood(self.y)\n",
    "    def build_finetune_functions(self, datasets, batch_size, learning_rate, L1_param, L2_param, mom):\n",
    "        (train_set_x, train_set_y) = datasets[0]\n",
    "        (valid_set_x, valid_set_y) = datasets[1]\n",
    "        (test_set_x, test_set_y)   = datasets[2]\n",
    "        index = T.lvector('index')\n",
    "        gparams = T.grad( self.finetune_cost  + L1_param * abs(self.layers[-1].W).sum() + L2_param * (self.layers[-1].W **2).sum(), self.params)\n",
    "        updates1 = OrderedDict()\n",
    "        for param, gparam, gparam_mom in zip(self.params, gparams, self.gparams_mom):\n",
    "            updates1[gparam_mom] = mom * gparam_mom - learning_rate * gparam\n",
    "            updates1[param] = param + updates1[gparam_mom]\n",
    "        train_fn = theano.function(\n",
    "            inputs =[index],\n",
    "            outputs=self.finetune_cost,\n",
    "            updates=updates1,\n",
    "            givens={ self.x: train_set_x[index],\n",
    "                     self.y: train_set_y[index]}  )\n",
    "        valid_pred_fn = theano.function(\n",
    "            inputs = [index],\n",
    "            outputs=self.layers[-1].p_y_given_x,\n",
    "            givens ={self.x: valid_set_x[index]} )\n",
    "        valid_y_fn = theano.function(\n",
    "            inputs = [index],\n",
    "            outputs= self.y,\n",
    "            givens ={self.y: valid_set_y[index] } )\n",
    "        test_pred_fn = theano.function(\n",
    "            inputs = [index],\n",
    "            outputs=self.layers[-1].p_y_given_x,\n",
    "            givens ={self.x: test_set_x[index]} )\n",
    "        test_y_fn = theano.function(\n",
    "            inputs = [index],\n",
    "            outputs= self.y,\n",
    "            givens ={self.y: test_set_y[index] } )\n",
    "        def getVals( fn, IDX, n_exp, batch_size ):\n",
    "            vals = list()\n",
    "            n_batches = n_exp/ batch_size\n",
    "            resid     = n_exp  - (n_batches * batch_size)\n",
    "            cnt = int(math.ceil(batch_size / n_exp))\n",
    "            for i in range(n_batches):\n",
    "                vals+= fn(IDX[i*batch_size:(i+1)*batch_size]).tolist()\n",
    "            if cnt <= 1 and resid !=0:\n",
    "                val = fn(IDX[(n_batches-1)*batch_size+resid:(n_batches*batch_size)+resid])\n",
    "                vals+= val[(batch_size-resid):batch_size].tolist()\n",
    "            if cnt > 1:\n",
    "                IDX_ = IDX\n",
    "                for i in range(cnt-1):\n",
    "                    IDX_ = numpy.concatenate((IDX_, IDX))\n",
    "                val = fn(IDX_[0: batch_size])\n",
    "                vals += val[range(n_exp)].tolist()\n",
    "            return vals\n",
    "        n_valid_exp = valid_set_x.get_value(borrow=True).shape[0]\n",
    "        n_test_exp  =  test_set_x.get_value(borrow=True).shape[0]\n",
    "        def valid_check():\n",
    "            idx = numpy.random.permutation(range(n_valid_exp))\n",
    "            valid_y    = getVals( valid_y_fn,   idx, n_valid_exp, batch_size )\n",
    "            valid_pred = getVals( valid_pred_fn,idx, n_valid_exp, batch_size )\n",
    "            return valid_y, valid_pred\n",
    "        def test_check():\n",
    "            idx = numpy.random.permutation(range(n_test_exp))\n",
    "            test_y     = getVals( test_y_fn,    idx, n_test_exp, batch_size )\n",
    "            test_pred  = getVals( test_pred_fn, idx, n_test_exp, batch_size )\n",
    "            return test_y, test_pred\n",
    "        return train_fn, valid_check, test_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelFile = 'data/mhc1-pan.pkl.gz'\n",
    "# modelFile = 'data/mhc2-pan.pkl.gz'\n",
    "# modelFile = 'data/tcr1-pan.pkl.gz'\n",
    "# modelFile = 'data/tcr2-pan.pkl.gz'\n",
    "testdata = 'temp/class1_input.dat.pkl.gz'\n",
    "predFile = 'temp/class1_mhcbinding_result.txt'\n",
    "\n",
    "#print '\\n', 'Model file: ', modelFile, '\\n'\n",
    "#print 'Test data: ', testdata, '\\n'\n",
    "#print 'Prediction result: ', predFile, '\\n'\n",
    "\n",
    "classifier\t= cPickle.load(gzip.open(modelFile))\n",
    "datasets = Load_data_ind(testdata)\n",
    "test_set_x, test_set_y = datasets[0]\n",
    "\n",
    "get_y\t= theano.function([], test_set_y)\n",
    "y_      = get_y()\n",
    "x_      = np.asarray(test_set_x.get_value(borrow=True) , dtype='float32')\n",
    "\n",
    "batch_size=int(10)\n",
    "predict_model = theano.function( inputs = [classifier.x], outputs= classifier.layers[-1].output )\n",
    "n_exp = ( x_.shape[0] )\n",
    "cnt = int(math.ceil(batch_size / n_exp))\n",
    "n_batches = n_exp / batch_size\n",
    "resid = n_exp - (n_batches * batch_size)\n",
    "y_answer = list()\n",
    "y_pred = list()\n",
    "for index in range(n_batches):\n",
    "\txx = x_[index * batch_size: (index + 1) * batch_size]\n",
    "\tres = predict_model(xx)\n",
    "\ty_pred += res[range(batch_size)].tolist()\n",
    "\n",
    "if cnt <= 1 and resid != 0:\n",
    "\txx = x_[(n_batches-1) * batch_size + resid: (n_batches*batch_size)+resid]\n",
    "\tres = predict_model(xx)\n",
    "\ty_pred += res[(batch_size-resid):batch_size].tolist()\n",
    "\n",
    "if cnt > 1:\n",
    "\txx = x_\n",
    "\tfor i in range(cnt-1):\n",
    "\t\txx = np.concatenate((xx, x_))\n",
    "\tres = predict_model(xx[0: batch_size])\n",
    "\ty_pred += res[range(n_exp)].tolist()\n",
    "\n",
    "fout = open(predFile,'w')\n",
    "#tids = ['\\t'.join(x.strip().split('\\t')[:-1]) for x in open(testdata.split('/')[-1].split('.')[0]+'.'+testdata.split('/')[-1].split('.')[1]).readlines()]\n",
    "#tids = ['\\t'.join(x.strip().split('\\t')[:-1]) for x in open(testdata.split('.')[0]+'.'+testdata.split('.')[1]).readlines()]\n",
    "#tids = ['\\t'.join(x.strip().split('\\t')[:-1]) for x in open(testdata).readlines()]\n",
    "tids = ['\\t'.join(x.strip().split('\\t')[:-1]) for x in open(testdata.split('/')[-1].split('.')[0]+'.'+testdata.split('/')[-1].split('.')[1]).readlines()]\n",
    "#print(tids)\n",
    "for i in range(len(y_)):\n",
    "#        print(tids[i])\n",
    "\tfout.write(tids[i]+'\\t'+str(y_pred[i])+'\\n')\n",
    "fout.close()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
