{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import gzip\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 데이터 전처리 함수\n",
    "def match_dat(afflst, hladic, aadic):\n",
    "    seqlst = []\n",
    "    tablst = []\n",
    "    header = []\n",
    "    for affin in afflst:\n",
    "        affstr = affin.strip().split('\\t')\n",
    "        if affstr[0] in hladic:\n",
    "            hlaseq = hladic[affstr[0]]\n",
    "            aaseq = affstr[1]\n",
    "            tmp = []\n",
    "            tmp0 = []\n",
    "            for hlain in hlaseq:\n",
    "                for aain in aaseq:\n",
    "                    if hlain == 'X' or aain == 'X':\n",
    "                        tmp0.append([float(0)])\n",
    "                    elif hlain == '*' or hlain == '.' or aain == 'U':\n",
    "                        tmp0.append([float(0)])\n",
    "                    elif aain == 'J':\n",
    "                        aa1 = aadic[hlain, 'L']\n",
    "                        aa2 = aadic[hlain, 'I']\n",
    "                        aamax = max(aa1, aa2)\n",
    "                        tmp0.append([float(aamax)])\n",
    "                    elif aain == 'Z':\n",
    "                        aa1 = aadic[hlain, 'Q']\n",
    "                        aa2 = aadic[hlain, 'E']\n",
    "                        aamax = max(aa1, aa2)\n",
    "                        tmp0.append([float(aamax)])\n",
    "                    elif aain == 'B':\n",
    "                        aa1 = aadic[hlain, 'D']\n",
    "                        aa2 = aadic[hlain, 'N']\n",
    "                        aamax = max(aa1, aa2)\n",
    "                        tmp0.append([float(aamax)])\n",
    "                    else:\n",
    "                        tmp0.append([aadic[hlain, aain]])\n",
    "                tmp.append(tmp0)\n",
    "                tmp0 = []\n",
    "            seqlst.append(list(zip(*tmp)))\n",
    "            tablst.append(int(affstr[2]))\n",
    "            header.append((affstr[0], affstr[1]))\n",
    "    seqarray0 = np.array(seqlst, dtype=np.float32)\n",
    "    a_seq2 = seqarray0.reshape(seqarray0.shape[0], seqarray0.shape[1] * seqarray0.shape[2])\n",
    "    a_lab2 = np.array(tablst, dtype=np.float32)\n",
    "    return (a_seq2, a_lab2), header\n",
    "\n",
    "def header_output(lstin, outname):\n",
    "    with open(outname, 'w') as outw:\n",
    "        for lin in lstin:\n",
    "            outw.write('\\t'.join(lin) + '\\n')\n",
    "\n",
    "def modify_matrix(affydatin_test, seqdatin, outfile):\n",
    "    hladicin = {x.strip().split('\\t')[0]: x.strip().split('\\t')[1] for x in open(seqdatin).readlines()}\n",
    "    aalst = open('data/Calpha.txt').readlines()\n",
    "    aadicin = {}\n",
    "    aaseq0 = aalst[0].strip().split('\\t')\n",
    "    for aain in aalst[1:]:\n",
    "        aastr = aain.strip().split('\\t')\n",
    "        for i in range(1, len(aastr)):\n",
    "            aadicin[aaseq0[i-1], aastr[0]] = float(aastr[i])\n",
    "    afflst = open(affydatin_test).readlines()\n",
    "    d, test_header = match_dat(afflst, hladicin, aadicin)\n",
    "    with gzip.open(outfile, 'wb') as f:\n",
    "        pickle.dump(d, f, protocol=2)\n",
    "    header_output(test_header, affydatin_test + '.header')\n",
    "\n",
    "datname = 'data/class1_input.dat'\n",
    "modify_matrix(datname, 'data/All_prot_alignseq_C_369.dat', 'temp/class1_input.dat.pkl.gz')\n",
    "print('The running is completed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드 함수\n",
    "def shared_dataset(data_xy):\n",
    "    data_x, data_y = data_xy\n",
    "    tensor_x = torch.tensor(data_x, dtype=torch.float32)\n",
    "    tensor_y = torch.tensor(data_y, dtype=torch.int64)\n",
    "    return tensor_x, tensor_y\n",
    "\n",
    "def load_data(dataset):\n",
    "    print('... loading data')\n",
    "    with gzip.open(dataset, 'rb') as f:\n",
    "        train_set, valid_set, test_set = pickle.load(f)\n",
    "    train_set_x, train_set_y = shared_dataset(train_set)\n",
    "    valid_set_x, valid_set_y = shared_dataset(valid_set)\n",
    "    test_set_x, test_set_y = shared_dataset(test_set)\n",
    "    return (train_set_x, train_set_y), (valid_set_x, valid_set_y), (test_set_x, test_set_y)\n",
    "\n",
    "def load_data_ind(dataset):\n",
    "    print('... loading data')\n",
    "    with gzip.open(dataset, 'rb') as f:\n",
    "        test_set = pickle.load(f)\n",
    "    test_set_x, test_set_y = shared_dataset(test_set)\n",
    "    return test_set_x, test_set_y\n",
    "\n",
    "def load_npdata(dataset):\n",
    "    print('... loading data')\n",
    "    datasets = np.load(dataset)\n",
    "    test_set_x = datasets['test_seq']\n",
    "    test_set_y = datasets['test_lab']\n",
    "    test_set = (test_set_x, test_set_y)\n",
    "    test_set_x, test_set_y = shared_dataset(test_set)\n",
    "    return test_set_x, test_set_y\n",
    "\n",
    "# 모델 정의\n",
    "class LeNetConvPoolLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, pool_size):\n",
    "        super(LeNetConvPoolLayer, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n",
    "        self.pool = nn.MaxPool2d(pool_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        return x\n",
    "\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, n_in, n_out):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(n_in, n_out)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = self.sigmoid(x).flatten()\n",
    "        return x\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, in_dim, nkerns, filtsize, poolsize, hidden):\n",
    "        super(CNN, self).__init__()\n",
    "        self.layer0 = LeNetConvPoolLayer(1, nkerns[0], filtsize[0], poolsize[0])\n",
    "        self.layer1 = LeNetConvPoolLayer(nkerns[0], nkerns[1], filtsize[1], poolsize[1])\n",
    "        # self.fc input size is calculated as: output channels * output height * output width\n",
    "        conv_output_size = self._get_conv_output(in_dim, nkerns, filtsize, poolsize)\n",
    "        self.fc = LogisticRegression(conv_output_size, hidden)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer0(x)\n",
    "        x = self.layer1(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    def _get_conv_output(self, shape, nkerns, filtsize, poolsize):\n",
    "        o = torch.zeros(1, *shape)\n",
    "        o = self.layer0(o)\n",
    "        o = self.layer1(o)\n",
    "        return int(np.prod(o.size()))\n",
    "\n",
    "# 데이터 로드\n",
    "train_set, valid_set, test_set = load_data('temp/class1_input.dat.pkl.gz')\n",
    "train_loader = DataLoader(TensorDataset(*train_set), batch_size=10, shuffle=True)\n",
    "valid_loader = DataLoader(TensorDataset(*valid_set), batch_size=10, shuffle=False)\n",
    "test_loader = DataLoader(TensorDataset(*test_set), batch_size=10, shuffle=False)\n",
    "\n",
    "# 모델 초기화\n",
    "model = CNN(in_dim=(28, 28), nkerns=[20, 50], filtsize=[(5, 5), (5, 5)], poolsize=[(2, 2), (2, 2)], hidden=1)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습 및 평가 함수\n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.view(inputs.size(0), 1, 28, 28)  # Assuming input size is 28x28\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader)}')\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.view(inputs.size(0), 1, 28, 28)  # Assuming input size is 28x28\n",
    "            outputs = model(inputs)\n",
    "            preds = outputs > 0.5\n",
    "            y_true.extend(labels.numpy())\n",
    "            y_pred.extend(preds.numpy())\n",
    "    return y_true, y_pred\n",
    "\n",
    "# 모델 학습\n",
    "train_model(model, train_loader, criterion, optimizer, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 평가\n",
    "y_true, y_pred = evaluate_model(model, test_loader)\n",
    "\n",
    "# 예측 결과 저장\n",
    "with open('temp/class1_mhcbinding_result.txt', 'w') as f:\n",
    "    for true, pred in zip(y_true, y_pred):\n",
    "        f.write(f'{true}\\t{pred}\\n')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
