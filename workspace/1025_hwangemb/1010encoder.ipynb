{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of HLA alleles: 7045\n",
      "Number of samples: 102180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 8/6387 [00:03<50:40,  2.10it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 102\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sequence_representations\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# 인코딩된 시퀀스를 얻습니다.\u001b[39;00m\n\u001b[0;32m--> 102\u001b[0m encoded_sequences \u001b[38;5;241m=\u001b[39m \u001b[43mencode_sequences\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_converter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcommon_hla_sequence_A_or_B\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcommon_hla_sequence_C\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;66;03m# 인코딩된 결과를 저장합니다.\u001b[39;00m\n\u001b[1;32m    105\u001b[0m np\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhlaembedding.npy\u001b[39m\u001b[38;5;124m'\u001b[39m, [seq[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhla_embedding_diff\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m encoded_sequences])\n",
      "Cell \u001b[0;32mIn[1], line 72\u001b[0m, in \u001b[0;36mencode_sequences\u001b[0;34m(data_loader, batch_converter, model, common_hla_sequence_A_or_B, common_hla_sequence_C)\u001b[0m\n\u001b[1;32m     70\u001b[0m         hla_embeddings_diff\u001b[38;5;241m.\u001b[39mappend(hla_embedding \u001b[38;5;241m-\u001b[39m common_hla_embedding_C)\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 72\u001b[0m         hla_embedding \u001b[38;5;241m=\u001b[39m \u001b[43membed_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcleaned_hla_seqs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_converter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     73\u001b[0m         hla_embeddings_diff\u001b[38;5;241m.\u001b[39mappend(hla_embedding \u001b[38;5;241m-\u001b[39m common_hla_embedding_A_or_B)\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import esm\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ESM 모델과 배치 컨버터를 로드합니다.\n",
    "model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
    "batch_converter = alphabet.get_batch_converter()\n",
    "\n",
    "# 모델을 평가 모드로 설정하고 GPU로 이동\n",
    "model.eval()\n",
    "model.cuda()\n",
    "\n",
    "# DataProvider 클래스를 임포트하고 데이터 로더를 설정합니다.\n",
    "from dataprovider import DataProvider\n",
    "\n",
    "# 데이터 경로 설정\n",
    "epi_path = '../../data/deepneo/mhc1.testset.csv'\n",
    "hla_path = '../../data/deepneo/HLAseq.csv'\n",
    "\n",
    "# DataProvider 인스턴스 생성\n",
    "data_provider = DataProvider(epi_path, hla_path)\n",
    "\n",
    "# 데이터 로더 설정\n",
    "data_loader = DataLoader(data_provider, batch_size=16, shuffle=False, collate_fn=lambda x: x)\n",
    "\n",
    "# HLA 유형별 보편적인 서열\n",
    "common_hla_sequence_A_or_B = \"SHSMRYFYTSVSRPGRGEPRFIAVGYVDDTQFVRFDSDAASPRMEPRAPWIEQEGPEYWDRETQIVKANSQTDRESLRTLRGYYNQSEAGSHTIQRMYGCDVGPDGRLLRGYNQYAYDGKDYIALNEDLRSWTAADTAAQITQRKWEAARVAEQLRAYLEGTCVEWLRRYLENGKETLQRA******************************************************************************************************************************************\"  # HLAA 또는 HLAB 보편적인 시퀀스\n",
    "common_hla_sequence_C = \"MRYFYTAVSRPGRGEPRFIAVGYVDDTQFVRFDSDAASPRGEPRAPWVEQEGPEYWDRETQKYKRQAQADRVSLRNLRGYYNQSEAGSHTLQRMYGCDLGPDGRLLRGYDQSAYDGKDYIALNEDLRSWTAADTAAQITQRKWEAAREAEQLRAYLEGTCVEWLRRYLENGKETLQRAEPKTHVTHHPSDHEATLRCWALGFYPT**************DTELVETRPAGDGTFQKWAAVVVPSGEQRYTCHQHEGLEPLTL*W********************************************************************\"       # HLAC 보편적인 시퀀스\n",
    "\n",
    "def clean_hla_sequence(sequence):\n",
    "    \"\"\"HLA 서열에서 패딩된 '*' 문자를 제거합니다.\"\"\"\n",
    "    return sequence.replace('*', 'X')\n",
    "\n",
    "def embed_sequence(sequence, batch_converter, model):\n",
    "    \"\"\"서열을 ESM 모델로 임베딩하는 함수.\"\"\"\n",
    "    labels = [(\"sequence\", sequence)]\n",
    "    _, _, tokens = batch_converter(labels)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        results = model(tokens.cuda(), repr_layers=[33], return_contacts=False)\n",
    "    \n",
    "    return results[\"representations\"][33][0, 1: len(sequence) + 1].mean(0)\n",
    "\n",
    "def encode_sequences(data_loader, batch_converter, model, common_hla_sequence_A_or_B, common_hla_sequence_C):\n",
    "    sequence_representations = []\n",
    "    \n",
    "    # 보편적인 HLA 서열을 클린하고 임베딩 (A/B와 C 각각)\n",
    "    cleaned_common_hla_A_or_B = clean_hla_sequence(common_hla_sequence_A_or_B)\n",
    "    cleaned_common_hla_C = clean_hla_sequence(common_hla_sequence_C)\n",
    "    \n",
    "    common_hla_embedding_A_or_B = embed_sequence(cleaned_common_hla_A_or_B, batch_converter, model).cpu().numpy()\n",
    "    common_hla_embedding_C = embed_sequence(cleaned_common_hla_C, batch_converter, model).cpu().numpy()\n",
    "    \n",
    "    for batch in tqdm(data_loader):\n",
    "        hla_names, epi_seqs, targets, hla_seqs = zip(*batch)\n",
    "        \n",
    "        # HLA 서열에서 '*'를 제거합니다.\n",
    "        cleaned_hla_seqs = [clean_hla_sequence(hla_seq) for hla_seq in hla_seqs]\n",
    "        \n",
    "        # HLA 시퀀스를 각각 임베딩하고 보편적인 서열과 차이를 계산\n",
    "        hla_embeddings_diff = []\n",
    "        for i, hla_name in enumerate(hla_names):\n",
    "            try:\n",
    "                if hla_name.startswith(\"HLAC\"):\n",
    "                    hla_embedding = embed_sequence(cleaned_hla_seqs[i], batch_converter, model).cpu().numpy()\n",
    "                    hla_embeddings_diff.append(hla_embedding - common_hla_embedding_C)\n",
    "                else:\n",
    "                    hla_embedding = embed_sequence(cleaned_hla_seqs[i], batch_converter, model).cpu().numpy()\n",
    "                    hla_embeddings_diff.append(hla_embedding - common_hla_embedding_A_or_B)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing HLA sequence: {hla_name}\")\n",
    "                print(f\"Sequence: {hla_seqs[i]}\")\n",
    "                print(f\"Exception: {e}\")\n",
    "                continue  # 문제 발생 시 해당 시퀀스를 건너뜀\n",
    "        \n",
    "        # 길이 확인\n",
    "        if len(hla_embeddings_diff) != len(batch):\n",
    "            print(f\"Warning: hla_embeddings_diff 길이가 {len(hla_embeddings_diff)} 이고 batch 길이는 {len(batch)} 입니다. 일치하지 않음.\")\n",
    "            continue  # 일치하지 않으면 이 배치는 건너뜀\n",
    "\n",
    "        # Epitope 시퀀스를 각각 임베딩\n",
    "        epitope_embeddings = [embed_sequence(epi_seq, batch_converter, model).cpu().numpy() for epi_seq in epi_seqs]\n",
    "        \n",
    "        # 임베딩 결과를 저장\n",
    "        for i, (hla_name, epi_seq, target, hla_seq) in enumerate(batch):\n",
    "            sequence_representations.append({\n",
    "                'hla_name': hla_name,\n",
    "                'epi_seq': epitope_embeddings[i],  # Epitope 임베딩을 저장\n",
    "                'target': target,\n",
    "                'hla_seq': hla_seq,  # 보편적인 시퀀스와 차이를 저장\n",
    "                'hla_embedding_diff': hla_embeddings_diff[i],\n",
    "            })\n",
    "    \n",
    "    return sequence_representations\n",
    "\n",
    "\n",
    "# 인코딩된 시퀀스를 얻습니다.\n",
    "encoded_sequences = encode_sequences(data_loader, batch_converter, model, common_hla_sequence_A_or_B, common_hla_sequence_C)\n",
    "\n",
    "# 인코딩된 결과를 저장합니다.\n",
    "np.save('hlaembedding.npy', [seq['hla_embedding_diff'] for seq in encoded_sequences])\n",
    "\n",
    "# Epitope 임베딩도 저장합니다.\n",
    "np.save('epitopeembedding.npy', [seq['epi_seq'] for seq in encoded_sequences])\n",
    "\n",
    "# 추가적으로 메타데이터도 저장할 수 있습니다.\n",
    "meta_data = [{\n",
    "    'hla_name': seq['hla_name'],\n",
    "    'epi_seq': seq['epi_seq'],  # Epitope 임베딩을 메타데이터에 저장\n",
    "    'target': seq['target'],\n",
    "    'hla_seq': seq['hla_seq'],\n",
    "} for seq in encoded_sequences]\n",
    "\n",
    "meta_df = pd.DataFrame(meta_data)\n",
    "meta_df.to_csv('metadata.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
